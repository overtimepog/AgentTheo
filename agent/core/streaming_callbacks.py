"""
Streaming callbacks for LangGraph integration
Provides various callback handlers for streaming events
"""

from typing import Any, Dict, Optional, List
from langchain_core.callbacks import AsyncCallbackHandler
from langchain_core.outputs import LLMResult
from ..utils.logger import get_logger


class StreamingCallbackHandler(AsyncCallbackHandler):
    """Async callback handler for streaming LLM outputs"""
    
    def __init__(self, stream_callback: callable = None):
        """Initialize the streaming callback handler
        
        Args:
            stream_callback: Function to call with each token/chunk
        """
        super().__init__()
        self.logger = get_logger('streaming_callback')
        self.stream_callback = stream_callback
        self.tokens = []
        
    async def on_llm_new_token(self, token: str, **kwargs) -> None:
        """Called when a new token is generated by the LLM
        
        Args:
            token: The new token
            **kwargs: Additional arguments
        """
        self.tokens.append(token)
        
        if self.stream_callback:
            await self.stream_callback({
                "type": "token",
                "content": token,
                "metadata": kwargs
            })
            
    async def on_llm_start(
        self, 
        serialized: Dict[str, Any],
        prompts: List[str], 
        **kwargs
    ) -> None:
        """Called when LLM starts generating
        
        Args:
            serialized: Serialized LLM data
            prompts: Input prompts
            **kwargs: Additional arguments
        """
        self.logger.debug("LLM generation started")
        
        if self.stream_callback:
            await self.stream_callback({
                "type": "llm_start",
                "content": "",
                "metadata": {
                    "prompts": prompts,
                    **kwargs
                }
            })
    
    async def on_llm_end(self, response: LLMResult, **kwargs) -> None:
        """Called when LLM finishes generating
        
        Args:
            response: The complete LLM response
            **kwargs: Additional arguments
        """
        self.logger.debug("LLM generation completed")
        
        if self.stream_callback:
            await self.stream_callback({
                "type": "llm_end",
                "content": "".join(self.tokens),
                "metadata": {
                    "response": response,
                    **kwargs
                }
            })
            
        # Reset tokens
        self.tokens = []
    
    async def on_llm_error(self, error: Exception, **kwargs) -> None:
        """Called when LLM encounters an error
        
        Args:
            error: The exception that occurred
            **kwargs: Additional arguments
        """
        self.logger.error(f"LLM error: {error}")
        
        if self.stream_callback:
            await self.stream_callback({
                "type": "llm_error",
                "content": str(error),
                "metadata": kwargs
            })
    
    async def on_tool_start(
        self,
        serialized: Dict[str, Any],
        input_str: str,
        **kwargs
    ) -> None:
        """Called when a tool starts executing
        
        Args:
            serialized: Serialized tool data
            input_str: Tool input
            **kwargs: Additional arguments
        """
        tool_name = serialized.get("name", "unknown")
        self.logger.debug(f"Tool {tool_name} started")
        
        if self.stream_callback:
            await self.stream_callback({
                "type": "tool_start",
                "content": f"Executing {tool_name}...",
                "metadata": {
                    "tool": tool_name,
                    "input": input_str,
                    **kwargs
                }
            })
    
    async def on_tool_end(self, output: str, **kwargs) -> None:
        """Called when a tool finishes executing
        
        Args:
            output: Tool output
            **kwargs: Additional arguments
        """
        self.logger.debug("Tool execution completed")
        
        if self.stream_callback:
            await self.stream_callback({
                "type": "tool_end",
                "content": output,
                "metadata": kwargs
            })
    
    async def on_tool_error(self, error: Exception, **kwargs) -> None:
        """Called when a tool encounters an error
        
        Args:
            error: The exception that occurred
            **kwargs: Additional arguments  
        """
        self.logger.error(f"Tool error: {error}")
        
        if self.stream_callback:
            await self.stream_callback({
                "type": "tool_error",
                "content": str(error),
                "metadata": kwargs
            })


class ProgressCallbackHandler(AsyncCallbackHandler):
    """Callback handler for tracking task progress"""
    
    def __init__(self, progress_callback: callable = None):
        """Initialize the progress callback handler
        
        Args:
            progress_callback: Function to call with progress updates
        """
        super().__init__()
        self.logger = get_logger('progress_callback')
        self.progress_callback = progress_callback
        self.steps_completed = 0
        self.total_steps = 0
        
    async def on_chain_start(
        self,
        serialized: Dict[str, Any],
        inputs: Dict[str, Any],
        **kwargs
    ) -> None:
        """Called when a chain starts executing
        
        Args:
            serialized: Serialized chain data
            inputs: Chain inputs
            **kwargs: Additional arguments
        """
        self.logger.debug("Chain execution started")
        
        if self.progress_callback:
            await self.progress_callback({
                "type": "progress",
                "content": "Task started",
                "metadata": {
                    "step": "start",
                    "completed": 0,
                    "total": 0
                }
            })
    
    async def on_chain_end(self, outputs: Dict[str, Any], **kwargs) -> None:
        """Called when a chain finishes executing
        
        Args:
            outputs: Chain outputs
            **kwargs: Additional arguments
        """
        self.logger.debug("Chain execution completed")
        
        if self.progress_callback:
            await self.progress_callback({
                "type": "progress",
                "content": "Task completed",
                "metadata": {
                    "step": "complete",
                    "completed": self.steps_completed,
                    "total": self.total_steps
                }
            })
    
    async def on_agent_action(self, action: Any, **kwargs) -> None:
        """Called when agent takes an action
        
        Args:
            action: The action being taken
            **kwargs: Additional arguments
        """
        self.steps_completed += 1
        
        if self.progress_callback:
            await self.progress_callback({
                "type": "progress",
                "content": f"Step {self.steps_completed}: {action}",
                "metadata": {
                    "step": self.steps_completed,
                    "action": str(action),
                    "completed": self.steps_completed,
                    "total": self.total_steps
                }
            })